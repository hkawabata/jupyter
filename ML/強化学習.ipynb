{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "designed-young",
   "metadata": {},
   "source": [
    "# 強化学習\n",
    "\n",
    "\n",
    "## 行動・状態・報酬\n",
    "\n",
    "エージェントが環境に対して **行動** し、その結果として **状態** と **報酬** を受け取る。\n",
    "\n",
    "- **行動 (Action)**：エージェントの振る舞い\n",
    "- **状態 (State)**：エージェントの置かれた状況\n",
    "- **即時報酬 (Immediate Reward)**：行動の「即時的な」良さ（= 後のことは考えない）。単に報酬とも呼ぶ\n",
    "\n",
    "- **ステップ**：エージェントが行動し、その結果環境から状態と即時報酬を受け取る1サイクル\n",
    "- **エピソード**：強化学習で解きたいタスクを開始してから終了するまでの期間。複数のステップの繰り返し。\n",
    "\n",
    "- **マルコフ性** (Markov property)：「未来の状態 $$s_{t+1}$$ は現在の状態 $$s_t$$ と現在の行動 $$a_t$$ のみを使って記述でき、それよりも過去の状態や行動には依存しない」という性質\n",
    "- **マルコフ決定過程** (Markov Decision Process, MDP)：\n",
    "- **方策 (Policy)**：エージェントの行動を決定するルール（関数）\n",
    "\n",
    "**強化学習を解く = 方策の最適化問題を解く**\n",
    "\n",
    "> **【NOTE】**\n",
    ">\n",
    "> 「取りうる状態・行動の個数が有限である**有限マルコフ決定過程**においては、最適な方策が少なくとも1つは存在する」ということが知られている\n",
    "\n",
    "## 即時報酬と収益\n",
    "\n",
    "すぐには大きな即時報酬を得られない行動でも、後々大きな即時報酬につながる可能性がある  \n",
    "→ **毎ステップでその時の即時報酬が最大となるように行動しても、最終的に得られる即時報酬の総和が最大化できるとは限らない**  \n",
    "→ ある時間ステップ以降に得られる即時報酬の累計 = **収益 (Return)** $$G_t$$ で評価する\n",
    "\n",
    "<img width=\"700\" alt=\"図1\" src=\"https://user-images.githubusercontent.com/13412823/154613038-de25b35b-28ee-40a3-843e-f84af244f049.png\">\n",
    "\n",
    "収益（= 報酬の累計）について、\n",
    "\n",
    "- 収益の正確な値は、エピソードが終了してみないと分からない\n",
    "- しかし、エージェントの立場としては、その時々で行動を選択する段階で収益を知りたい\n",
    "\n",
    "→ 見込みの値を見積もるしかない  \n",
    "→ 今よりも未来であるほど見積もりは不確かになるので、累計を取る際に **割引率** $$0 \\lt \\gamma \\lt 1$$ をかけて和を取る\n",
    "\n",
    "$$\n",
    "G_t\n",
    "\\equiv r_{t+1} +  \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots\n",
    "= \\displaystyle \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1}\n",
    "$$\n",
    "\n",
    "式変形して再帰的な式で表すこともできる：\n",
    "\n",
    "$$\n",
    "G_t = r_{t+1} +  \\gamma G_{t+1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 状態価値と状態行動価値\n",
    "\n",
    "前述の見込み収益 $$R_t$$ を計算するには、将来の即時報酬 $$r_{t+1}, r_{t+2}, \\cdots$$ の値がわかっている必要がある。  \n",
    "実際の即時報酬は行動してみないと分からないため、現実的ではない。\n",
    "\n",
    "→ 確率に基づいて期待値を計算する\n",
    "\n",
    "$$E$$ で期待値計算を表現すると、状態 $$s$$ における収益の期待値 = **状態価値 (State Value)** は\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V^\\pi(s_t) &\\equiv& E \\left( r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots \\right) \\\\\n",
    "&=& E \\left( r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\right)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "と再帰的に表せる。\n",
    "\n",
    "期待値は\n",
    "\n",
    "- 行動選択確率 $$\\pi \\left( a \\middle| s \\right)$$：方策 $$\\pi$$ のもとで、状態 $$s$$ が与えられたとき、エージェントが行動 $$a$$ を取る確率\n",
    "- 状態遷移確率 $$P \\left( s' \\middle| s, a \\right)$$：状態 $$s$$ においてエージェントが行動 $$a$$ を取ったとき、状態 $$s'$$ に遷移する確率\n",
    "\n",
    "をかけて全ての $$a, s'$$ で和を取ることで計算できるから、$$r_{t+1}$$ を「状態 $$s$$ から 状態 $$s'$$ に遷移したときの即時報酬」を表す関数 $$R(s, s')$$ で置き換えて\n",
    "\n",
    "$$\n",
    "V^\\pi(s) =\n",
    "\\sum_{a} \\pi \\left( a \\middle| s \\right) \\sum_{s'} P \\left(s' \\middle| s, a \\right)\n",
    "\\left( R(s, s') + \\gamma V^{\\pi}(s') \\right) \\\\\n",
    "$$\n",
    "\n",
    "これを **ベルマン方程式** (Bellman Equation) と呼ぶ。\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "\n",
    "- **状態価値** $$V^\\pi(s)$$：ある状態 $$s$$ から方策 $$\\pi$$ に従って行動し続けた場合にどれだけの収益を得られそうかの期待値\n",
    "- **状態行動価値** $$Q^\\pi(s, a)$$：ある状態 $$s$$ である行動 $$a$$ を取り、その後方策 $$\\pi$$ に従って行動し続けた場合にどれだけの収益を得られそうかの期待値\n",
    "\n",
    "将来の即時報酬\n",
    "\n",
    "遅いタイミングで得られた報酬に減少補正をかけるため、**割引率** $$0 \\lt \\gamma \\lt 1$$ を導入して定式化する：\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = E \\left( r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots ; s_t = s \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^\\pi(s, a)\n",
    "= E \\left( r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots ; s_t = s, a_t = a \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "effective-tulsa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 7]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "迷路問題を強化学習で解く\n",
    "\n",
    "xoogxxxx\n",
    "ooxxooxs\n",
    "xoxooxxo\n",
    "ooxooooo\n",
    "xoooxxox\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from enum import auto, Enum\n",
    "\n",
    "class Action(Enum):\n",
    "    RIGHT = auto()\n",
    "    LEFT = auto()\n",
    "    UP = auto()\n",
    "    DOWN = auto()\n",
    "\n",
    "\n",
    "class State(Enum):\n",
    "    START = auto()\n",
    "    GOAL = auto()\n",
    "    AISLE = auto()\n",
    "    WALL = auto()\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, maze):\n",
    "        self.maze = np.array(maze)\n",
    "        self.size = self.maze.shape\n",
    "        start = np.where(self.maze == 's')\n",
    "        goal = np.where(self.maze == 'g')\n",
    "        self.start = np.array([start[0][0], start[1][0]])\n",
    "        self.goal = np.array([goal[0][0], goal[1][0]])\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        env : 環境\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.position = env.start\n",
    "\n",
    "    def policy(self):\n",
    "        pass\n",
    "    \n",
    "    def move(self):\n",
    "        pass\n",
    "\n",
    "def main():\n",
    "    maze = [['x','o','o','g','x','x','x','x'],\n",
    "            ['o','o','x','x','o','o','x','s'],\n",
    "            ['x','o','x','o','o','x','x','o'],\n",
    "            ['o','o','x','o','o','o','o','o'],\n",
    "            ['x','o','o','o','x','x','o','x']]\n",
    "    env = Environment(maze)\n",
    "    agent = Agent(env)\n",
    "    print(agent.position)\n",
    "\n",
    "\n",
    "def R(s):\n",
    "    if s == 'g':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class Action:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-hometown",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
