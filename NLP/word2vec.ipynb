{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "particular-steering",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "## CBOW モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifteen-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "\n",
    "class MLPLayer(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MatMulIn(MLPLayer):\n",
    "    def __init__(self, W):\n",
    "        self.W = W\n",
    "        self.dW = None\n",
    "        self.A_list = []\n",
    "    \n",
    "    def forward(self, A_list, is_training=False):\n",
    "        out = np.zeros((A_list[0].shape[0], self.W.shape[1]))\n",
    "        for A in A_list:\n",
    "            out += np.dot(A, self.W)\n",
    "        out /= len(A_list)\n",
    "        if is_training:\n",
    "            self.A_list = A_list\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        #dA = np.dot(dout, self.W)\n",
    "        dA = np.dot(dout, self.W.T)\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        for A in self.A_list:\n",
    "            self.dW += np.dot(dout.T, A).T\n",
    "        self.dW /= len(self.A_list)\n",
    "        return dA\n",
    "\n",
    "    \n",
    "class MatMulOut(MLPLayer):\n",
    "    def __init__(self, W):\n",
    "        self.W = W\n",
    "        self.dW = None\n",
    "        self.A = None\n",
    "    \n",
    "    def forward(self, A, is_training=False):\n",
    "        out = np.dot(A, self.W)\n",
    "        if is_training:\n",
    "            self.A = A\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        #dA = np.dot(dout, self.W)\n",
    "        dA = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(dout.T, self.A).T\n",
    "        \n",
    "        self.A = None\n",
    "        return dA\n",
    "\n",
    "\n",
    "class Affine(MLPLayer):\n",
    "    def __init__(self, W, b):\n",
    "        # モデルの重み\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        # 重みの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        # Adam 用パラメータ\n",
    "        self.v_W = np.zeros(W.shape)\n",
    "        self.v_b = np.zeros(b.shape)\n",
    "        self.h_W = np.zeros(W.shape)\n",
    "        self.h_b = np.zeros(b.shape)\n",
    "        # 誤差逆伝播用の中間データ\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, A, is_training=False):\n",
    "        out = np.dot(A, self.W.T) + self.b\n",
    "        \n",
    "        if is_training:\n",
    "            self.A = A\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dA = np.dot(dout, self.W)\n",
    "        self.dW = np.dot(dout.T, self.A)\n",
    "        self.db = dout.sum(axis=0)\n",
    "        \n",
    "        self.A = None\n",
    "        return dA\n",
    "\n",
    "\n",
    "class SoftMax(MLPLayer):\n",
    "    def __init__(self):\n",
    "        # 誤差逆伝播用の中間データ\n",
    "        self.Y = None\n",
    "    \n",
    "    def forward(self, Z, is_training=False):\n",
    "        Z_exp = np.exp(Z)\n",
    "        Y = (Z_exp.T / Z_exp.sum(axis=1)).T\n",
    "        \n",
    "        if is_training:\n",
    "            self.Y = Y\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dZ = self.Y * (dout.T - np.sum(self.Y * dout, axis=1)).T\n",
    "        \n",
    "        self.Y = None\n",
    "        return dZ\n",
    "\n",
    "\n",
    "class CostCalculation(MLPLayer):\n",
    "    def __init__(self):\n",
    "        # 誤差逆伝播用の中間データ\n",
    "        self.Y_predict = None\n",
    "        self.Y_correct = None\n",
    "    \n",
    "    def forward(self, Y_predict, Y_correct, is_training=False):\n",
    "        cost = - np.sum(Y_correct * np.log(Y_predict) + (1.0 - Y_correct) * np.log(1.0 - Y_predict), axis=1)\n",
    "        cost = np.average(cost)\n",
    "        \n",
    "        if is_training:\n",
    "            self.Y_predict = Y_predict\n",
    "            self.Y_correct = Y_correct\n",
    "        return cost\n",
    "    \n",
    "    def backward(self, dout=1.0):\n",
    "        batch_size = self.Y_predict.shape[0]\n",
    "        dA = (self.Y_predict - self.Y_correct) / (self.Y_predict * (1.0 - self.Y_predict)) / batch_size\n",
    "        \n",
    "        self.Y_predict = None\n",
    "        self.Y_correct = None\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "resistant-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.08761368994783\n",
      "100 2.2873474098199695\n",
      "200 1.9638422770270292\n",
      "300 1.855970732682135\n",
      "400 1.813344142754579\n",
      "500 1.7911205996989479\n",
      "600 1.7794971592210873\n",
      "700 1.7734369246666375\n",
      "800 1.7699963461606263\n",
      "900 1.7678204917178715\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\t[ 2.44417187 -0.16361257  1.54876979 -0.09574359  1.68049699]\n",
      "say\t[-1.48061798 -1.2480011   1.11905655 -0.51744905 -1.9751576 ]\n",
      "hello\t[-0.19484317  1.16595252 -0.16848114 -1.58234769  2.42925657]\n",
      ".\t[-2.40291285 -0.11185574 -0.26385354 -1.24729914 -1.1322048 ]\n",
      "You\t[ 0.16016848  0.65640077 -0.76909549 -0.53162023 -0.17273062]\n",
      "goodbye\t[ 0.53985685  1.03910594  0.87179241 -1.77569526  1.03934756]\n",
      "He\t[ 0.02909822  0.2171231  -1.23939198 -0.90455903  1.78767673]\n",
      "She\t[-0.84193696  1.36511792 -0.5258273  -1.69574773  0.47889015]\n"
     ]
    }
   ],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, epochs=10, eta=0.1, n_hidden=5):\n",
    "        self.word2index = {}\n",
    "        self.index2word = []\n",
    "        self.word_cnt = 0\n",
    "        self.n_hidden = n_hidden\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.cost = []\n",
    "        \n",
    "    def fit(self, words):\n",
    "        one_hot = self.words2one_hot(words)\n",
    "        context_vector_list, correct_labels = self.one_hot2context_vector(one_hot)\n",
    "        self.W_in = np.random.randn(self.word_cnt, self.n_hidden)\n",
    "        self.W_out = np.random.randn(self.n_hidden, self.word_cnt)\n",
    "        self.layers = [\n",
    "            MatMulIn(self.W_in),\n",
    "            MatMulOut(self.W_out),\n",
    "            SoftMax()\n",
    "        ]\n",
    "        for t in range(self.epochs):\n",
    "            # フォワードプロパゲーション\n",
    "            out = context_vector_list\n",
    "            for l in self.layers:\n",
    "                out = l.forward(out, is_training=True)\n",
    "            self.cost_layer = CostCalculation()\n",
    "            cost = self.cost_layer.forward(out, correct_labels, is_training=True)\n",
    "            self.cost.append(cost)\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            dout = self.cost_layer.backward()\n",
    "            for l in self.layers[::-1]:\n",
    "                dout = l.backward(dout)\n",
    "            \n",
    "            self.__update()\n",
    "            \n",
    "            if t % 100 == 0:\n",
    "                print(t, cost)\n",
    "\n",
    "    def words2one_hot(self, words):\n",
    "        words_index = []\n",
    "        for w in words:\n",
    "            if w not in self.word2index:\n",
    "                self.word2index[w] = self.word_cnt\n",
    "                self.index2word.append(w)\n",
    "                self.word_cnt += 1\n",
    "            words_index.append(self.word2index[w])\n",
    "        one_hot = np.zeros((len(words), self.word_cnt))\n",
    "        for i in range(len(words)):\n",
    "            one_hot[i][words_index[i]] = 1.0\n",
    "        return one_hot\n",
    "    \n",
    "    def one_hot2context_vector(self, one_hot, n_context_backward=1, n_context_forward=1):\n",
    "        n_A = n_context_backward + n_context_forward\n",
    "        A_list = []\n",
    "        for _ in range(n_A):\n",
    "            A_list.append([])\n",
    "        correct_labels = one_hot[n_context_backward:-n_context_forward]\n",
    "        for i in range(n_context_backward, len(one_hot)-n_context_forward):\n",
    "            cnt = 0\n",
    "            for j in range(i-n_context_backward, i+n_context_forward+1):\n",
    "                if j != i:\n",
    "                    A_list[cnt].append(one_hot[j])\n",
    "                    cnt += 1\n",
    "        return [np.array(A) for A in A_list], correct_labels\n",
    "\n",
    "    def __update(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, MatMulIn):\n",
    "                layer.W -= self.eta * layer.dW\n",
    "            elif isinstance(layer, MatMulOut):\n",
    "                layer.W -= self.eta * layer.dW\n",
    "\n",
    "w2v = Word2Vec(epochs=1000, n_hidden=5)\n",
    "#w2v.fit(['I', 'say', 'hello', ',', 'you', 'say', 'goodbye', '.'])\n",
    "w2v.fit('I say hello . You say goodbye . He say hello . She say goodbye .'.split())\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(range(len(w2v.cost)), w2v.cost)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(len(w2v.index2word)):\n",
    "    print('{}\\t{}'.format(w2v.index2word[i], w2v.layers[0].W[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-sixth",
   "metadata": {},
   "source": [
    "## 改良版 CBOW モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "japanese-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.W = W\n",
    "        self.dW = None\n",
    "        self.idx_list = []\n",
    "    \n",
    "    def forward(self, A_list, is_training=False):\n",
    "        print('--- Embedding: forward')\n",
    "        batch_size = A_list[0].shape[0]\n",
    "        context_size = len(A_list)\n",
    "        out = np.zeros((batch_size, self.W.shape[1]))\n",
    "        idx_list = []\n",
    "        for i in range(context_size):\n",
    "            A = A_list[i]\n",
    "            idx = np.argmax(A, axis=1)\n",
    "            idx_list.append(idx)\n",
    "            out += self.W[idx]\n",
    "        out /= context_size\n",
    "        if is_training:\n",
    "            self.idx_list = idx_list\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        print('--- Embedding: backward')\n",
    "        context_size = len(self.idx_list)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        for idx in self.idx_list:\n",
    "            for i, word_id in enumerate(idx):\n",
    "                self.dW[word_id] += dout[i]\n",
    "            self.dW *= context_size\n",
    "        self.idx_list = []\n",
    "        return None\n",
    "\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, neg_sample_size=3):\n",
    "        \"\"\"\n",
    "        W : 重み\n",
    "        corpus : 単語 ID の配列からなるコーパス # one-hot ベクトルから成るコーパス\n",
    "        power : ネガティブサンプリングにおいて、出現頻度の小さい単語を優遇（1未満の正数、ゼロに近いほど優遇）\n",
    "        neg_sample_size : 負例を何件サンプリングするか\n",
    "        \"\"\"\n",
    "        self.W = W\n",
    "        #self.power = power\n",
    "        self.neg_sample_size = neg_sample_size\n",
    "        \n",
    "        #u, counts = np.unique(np.argmax(corpus, axis=1), return_counts=True)\n",
    "        u, counts = np.unique(corpus, return_counts=True)\n",
    "        #p = np.zeros(corpus.shape[1])\n",
    "        p = np.zeros_like(u)\n",
    "        for i in range(len(u)):\n",
    "            p[u[i]] = counts[i]\n",
    "        # 出現頻度の小さい単語を少し出やすくする\n",
    "        p = np.power(p, power)\n",
    "        p = p / np.sum(p)\n",
    "        self.p = p\n",
    "        \n",
    "        self.sigmoid_loss_layers = [SigmoidLoss() for _ in range(neg_sample_size+1)]\n",
    "        \n",
    "    def forward(self, h, word_ids_answer):\n",
    "        \"\"\"\n",
    "        h : 隠れ層のデータ\n",
    "        word_ids_answer : 正解単語の id リスト\n",
    "        \"\"\"\n",
    "        # 出力層の正解単語成分\n",
    "        out_positive = self.W[:, word_ids_answer]\n",
    "        # 出力層の不正解単語成分\n",
    "        for neg_sample_ids in self.__select_negative_sample_ids(word_ids_answer):\n",
    "            out_negative = self.W[:, neg_sample_ids]\n",
    "            \n",
    "        #for i, id_answer in enumerate(word_ids_answer):\n",
    "        #    cost = 0\n",
    "    \n",
    "    def __select_negative_sample_ids(self, word_ids_answer):\n",
    "        ids_candidates = np.array(set(range(self.W.shape[0])) - set(word_ids_answer))\n",
    "        p_new = self.p[ids_candidates]\n",
    "        return np.array([np.random.choice(ids_candidates, size=self.neg_sample_size, replace=False, p=p_new) for _ in range(self.neg_sample_size)])\n",
    "\n",
    "\n",
    "class SigmoidLoss:\n",
    "    def __init__(self):\n",
    "        self.p_expected = None\n",
    "    \n",
    "    def forward(self, out_actual, p_expected):\n",
    "        p_actual = 1.0 / (1.0 + np.exp(-out_actual))\n",
    "        loss = - (p_expected * np.log(p_actual) + (1.0-p_expected) * np.log(1.0-p_actual))\n",
    "        self.p_expected = p_expected\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1.0):\n",
    "        din = self.p_expected * (1.0 - self.p_expected) * dout\n",
    "        return din\n",
    "\n",
    "    \n",
    "class NegativeSamplingLossXXXXXX:\n",
    "    def __init__(self, W, corpus, power=0.75, neg_sample_size=3):\n",
    "        self.W = W\n",
    "        self.dW = None\n",
    "        self.neg_sample_size = neg_sample_size\n",
    "        u, counts = np.unique(np.argmax(corpus, axis=1), return_counts=True)\n",
    "        p = np.zeros(corpus.shape[1])\n",
    "        for i in range(len(u)):\n",
    "            p[u[i]] = counts[i]\n",
    "        p = np.power(p, power)\n",
    "        p = p / np.sum(p)\n",
    "        self.p = p\n",
    "        self.loss_layers = [SigmoidLoss() for _ in range(neg_sample_size+1)]\n",
    "    \n",
    "    def forward(self, H, answers):\n",
    "        \"\"\"\n",
    "        H : 隠れ層の出力\n",
    "        answers : 正解単語のインデックスの配列\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for i in range(len(answers)):\n",
    "            dot = H[i].dot(W[:, answers[i]])\n",
    "            p = self.sigmoid(dot)\n",
    "            loss += self.loss_layers[0].forward(p, 1.0)\n",
    "            words = np.random.choice(range(W.shape[1]), size=self.neg_sample_size, replace=False, p=self.p)\n",
    "            for j in range(len(words)):\n",
    "                dot = H[i].dot(W[:, words[j]])\n",
    "                p = self.sigmoid(dot)\n",
    "                loss += self.loss_layers[1+j].forward(p, 0.)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "    \n",
    "    def sigmoid(self, num):\n",
    "        return 1.0 / (1.0 + np.exp(-num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "happy-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Embedding: forward\n",
      "[1 9 6]\n",
      "[[0.  0.  0.  0.  0.2]\n",
      " [0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0.7]]\n",
      "[3 0 1]\n",
      "[[0.  0.  0.  0.  0.4]\n",
      " [0.  0.  0.  0.  0.1]\n",
      " [0.  0.  0.  0.  0.2]]\n",
      "--- Embedding: backward\n",
      "[1 9 6]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[3 0 1]\n",
      "[[0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.06]]\n",
      "[[0.    0.    0.    0.    0.03 ]\n",
      " [0.    0.    0.    0.    0.055]\n",
      " [0.    0.    0.    0.    0.045]]\n",
      "[[0.   0.   0.   0.   0.11]\n",
      " [0.   0.   0.   0.   0.21]\n",
      " [0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.06]\n",
      " [0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.18]\n",
      " [0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.22]]\n"
     ]
    }
   ],
   "source": [
    "A_list = [\n",
    "    np.array([\n",
    "        [0,1,0,0,0,0,0,0,0,0],\n",
    "        [0,0,0,0,0,0,0,0,0,1],\n",
    "        [0,0,0,0,0,0,1,0,0,0]\n",
    "    ]),\n",
    "    np.array([\n",
    "        [0,0,0,1,0,0,0,0,0,0],\n",
    "        [1,0,0,0,0,0,0,0,0,0],\n",
    "        [0,1,0,0,0,0,0,0,0,0]\n",
    "    ])\n",
    "]\n",
    "W = np.array(\n",
    "    [\n",
    "        [0,0,0,0,0.1],\n",
    "        [0,0,0,0,0.2],\n",
    "        [0,0,0,0,0.3],\n",
    "        [0,0,0,0,0.4],\n",
    "        [0,0,0,0,0.5],\n",
    "        [0,0,0,0,0.6],\n",
    "        [0,0,0,0,0.7],\n",
    "        [0,0,0,0,0.8],\n",
    "        [0,0,0,0,0.9],\n",
    "        [0,0,0,0,1.0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "e = Embedding(W)\n",
    "e_out = e.forward(A_list, True)\n",
    "e_dout = e_out * 0.1\n",
    "e.backward(e_dout)\n",
    "print(e_dout)\n",
    "print(e.dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-cleanup",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
